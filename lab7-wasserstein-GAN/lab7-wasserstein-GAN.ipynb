{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathantibbetts/anaconda2/lib/python2.7/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded.\n"
     ]
    }
   ],
   "source": [
    "# Nathan Tibbetts\n",
    "# Lab 7 - Improved Wasserstein Generative Adversarial Network\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from skimage.io import imsave\n",
    "from IPython.display import display, Image\n",
    "import PIL.Image as PILImage\n",
    "\n",
    "# Load Data:\n",
    "# Note: Unsupervised, so no split between test and train.\n",
    "TRAIN_DIR = \"img_align_celeba\"\n",
    "train_files  = listdir(TRAIN_DIR)\n",
    "\n",
    "N = len(train_files) # Number of images to use from the set.\n",
    "S = 64   # Mini-batch size - shared for both train and test.\n",
    "if (S > N): S = N\n",
    "\n",
    "example = imread(join(TRAIN_DIR, train_files[0]))\n",
    "RiH, RiW, C = example.shape\n",
    "W = 32 # Resolution to reduce to before starting\n",
    "H = W # int(W/float(RiW)*RiH)\n",
    "IN_SHAPE  = (H, W, C) # Used only to scale down the images we work with\n",
    "BATCH_IN_SHAPE  = (None, H, W, C)\n",
    "\n",
    "#print \"Accuracy Baseline is: %f\" % ((len(train_pos)/float(len(train_in_files)) * len(test_pos) +\n",
    "#                                    len(train_neg)/float(len(train_in_files)) * len(test_neg)) / len(test_in_files))\n",
    "#     % we say pos. * num real pos. cases + % we say neg. * num real neg. cases, all / by num. cases to make a %.\n",
    "\n",
    "#plt.imshow(example)\n",
    "#plt.imshow(np.array(np.uint8(resize(example,IN_SHAPE)*255)))\n",
    "#plt.imshow(np.array(resize(example,IN_SHAPE), dtype='float32'))\n",
    "#plt.imshow(resize(example,IN_SHAPE))\n",
    "#plt.imshow(example[RiH/4:3*RiH/4,(RiW-RiH/2)/2:RiW-(RiW-RiH/2)/2])\n",
    "#plt.imshow(resize(example[RiH/4:3*RiH/4,(RiW-RiH/2)/2:RiW-(RiW-RiH/2)/2],IN_SHAPE))\n",
    "\n",
    "print(\"Loading Data...\")                               # This part is for zooming in and squaring\n",
    "features = np.array([resize(imread(join(TRAIN_DIR, f))[RiH/4:3*RiH/4,(RiW-RiH/2)/2:RiW-(RiW-RiH/2)/2], IN_SHAPE)\n",
    "                     for f in train_files[:N]],  dtype='float32')\n",
    "\n",
    "# Whiten our data - zero mean and unit standard deviation\n",
    "features = (features - np.mean(features)) / np.std(features)\n",
    "# features = np.transpose(np.reshape(features,(N,3,32,32)),axes=[0,2,3,1])\n",
    "# features is (N, 1024, 1024, 3), or (batch_size, height, width, channels)\n",
    "print(\"Data Loaded.\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer Defining helper functions\n",
    "\n",
    "def leaky_relu(x, alpha=0.0):\n",
    "    return tf.maximum(alpha*x, x)\n",
    "\n",
    "# # Convolution Layer:\n",
    "# def conv( x, filter_size=3, stride=2, num_filters=64, relu=False, alpha=0.0, name=\"conv\" ):\n",
    "#     with tf.variable_scope(name):\n",
    "#         W = tf.get_variable(name + \"_W\",\n",
    "#                             shape=(filter_size,\n",
    "#                                    filter_size,\n",
    "#                                    x.get_shape().as_list()[3],\n",
    "#                                    num_filters),\n",
    "#                             initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "#         b = tf.get_variable(name + \"_b\",\n",
    "#                             shape=num_filters,\n",
    "#                             initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "#         # Notice the input x will be one minibatch at a time,\n",
    "#         #    but right now we're just building a computation graph!\n",
    "#         if (stride >= 1): h = tf.nn.conv2d(x, W, [1,int(stride),int(stride),1], padding=\"SAME\", name=name + \"_h\")\n",
    "#         else: h = tf.layers.conv2d_transpose(x, W, filter_size, [int(1.0/stride),int(1.0/stride)],\n",
    "#                                              padding=\"same\", name=name + \"_h\") #THIS IS BROKEN BECAUSE FILTERS SHOULD BE AN INTEGER!\n",
    "#         h = tf.nn.bias_add(h, b, name=name + \"_h\")\n",
    "#         if relu:\n",
    "#             h = tf.nn.relu(h, alpha)\n",
    "#         return h\n",
    "\n",
    "# # Fully-Connected Layer:\n",
    "# def fc( x, out_size=1000, relu=False, alpha=0.0, name=\"fc\" ):\n",
    "#     with tf.variable_scope(name):\n",
    "#         W = tf.get_variable(name + \"_W\",\n",
    "#                             shape=(x.get_shape().as_list()[-1],out_size), #first non-batch dimension\n",
    "#                             initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "#         b = tf.get_variable(name + \"_b\",\n",
    "#                             shape=(1,out_size),\n",
    "#                             initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "#         h = tf.matmul(x, W, name=name + \"_h\") + b #Assuming batch is 0th index, broadcast instead!\n",
    "#         #h = tf.nn.bias_add(h, b, name=\"h\")#(tf.reshape(h,[-1]), b, name=\"h\")\n",
    "#         if relu:\n",
    "#             h = leaky_relu(h, alpha)\n",
    "#         return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------#\n",
    "# Designing the Generator and Discriminator #\n",
    "#-------------------------------------------#\n",
    "\n",
    "# Hyper-Parameters:\n",
    "d = 100 # Dimensionality of random gaussian vector z\n",
    "c = 0.001 # Learning Rate\n",
    "lam = 10 # up to 0.05\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "ncritic = 1\n",
    "alpha = 0.0002\n",
    "v = 0.5 # Variance initialization factor\n",
    "\n",
    "batch_norm_decay = 0.9\n",
    "batch_norm_epsilon = 1e-5\n",
    "\n",
    "def generator(vec, reuse):\n",
    "    with tf.variable_scope(\"G_\", reuse=reuse):\n",
    "        G_h0 = tf.layers.dense(vec, 4*4*512, use_bias=True, name=\"fc0\",\n",
    "            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(factor=v),\n",
    "            bias_initializer=tf.contrib.layers.variance_scaling_initializer(factor=v))#[?,4*4*512]\n",
    "        G_h0 = tf.reshape(G_h0, [-1,4,4,512]) #[?,4,4,512]\n",
    "\n",
    "        G_h1 = tf.layers.conv2d_transpose(G_h0, 256, 3, strides=(2,2), padding=\"same\", name=\"conv1\",\n",
    "            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(factor=v),\n",
    "            bias_initializer=tf.contrib.layers.variance_scaling_initializer(factor=v))#[?,8,8,256]\n",
    "        G_h1 = leaky_relu(G_h1, alpha)\n",
    "        #G_h1 = tf.contrib.layers.batch_norm(G_h1) # NEED EXTRA IF STATEMENTS FOR SINGLE IMAGES!\n",
    "\n",
    "        G_h2 = tf.layers.conv2d_transpose(G_h1, 128, 3, strides=(2,2), padding=\"same\", name=\"conv2\",\n",
    "            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(factor=v),\n",
    "            bias_initializer=tf.contrib.layers.variance_scaling_initializer(factor=v))#[?,16,16,128]\n",
    "        G_h2 = leaky_relu(G_h2, alpha)\n",
    "        #G_h2 = tf.contrib.layers.batch_norm(G_h2)\n",
    "\n",
    "        G_h3 = tf.layers.conv2d_transpose(G_h2, 64,  3, strides=(2,2), padding=\"same\", name=\"conv3\",\n",
    "            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(factor=v),\n",
    "            bias_initializer=tf.contrib.layers.variance_scaling_initializer(factor=v))#[?,32,32,64]\n",
    "        G_h3 = leaky_relu(G_h3, alpha)\n",
    "        #G_h3 = tf.contrib.layers.batch_norm(G_h3)\n",
    "\n",
    "        G_h4 = tf.layers.conv2d_transpose(G_h3, 3,  3, strides=(1,1), padding=\"same\", name=\"conv4\",\n",
    "            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(factor=v),\n",
    "            bias_initializer=tf.contrib.layers.variance_scaling_initializer(factor=v))#[?,32,32,3]\n",
    "\n",
    "        G_out = tf.tanh(G_h4)\n",
    "        #G_out = (tf.tanh(G_h4) + 1)/2\n",
    "        #G_out = tf.maximum(0.0,tf.minimum(1.0,G_h4))\n",
    "        return G_out\n",
    "    \n",
    "def discriminator(images, reuse):\n",
    "    with tf.variable_scope(\"D_\", reuse=reuse):\n",
    "        D_h0 = tf.layers.conv2d(images, 64, 3, strides=(2,2), padding=\"same\", name=\"conv0\",\n",
    "            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(factor=v),\n",
    "            bias_initializer=tf.contrib.layers.variance_scaling_initializer(factor=v))#[?,16,16,64]\n",
    "        D_h0 = leaky_relu(D_h0, alpha)\n",
    "\n",
    "        D_h1 = tf.layers.conv2d(D_h0, 128, 3, strides=(2,2), padding=\"same\", name=\"conv1\",\n",
    "            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(factor=v),\n",
    "            bias_initializer=tf.contrib.layers.variance_scaling_initializer(factor=v))#[?,8,8,128]\n",
    "        D_h1 = leaky_relu(D_h1, alpha)\n",
    "        #D_h1 = tf.contrib.layers.batch_norm(D_h1)\n",
    "\n",
    "        D_h2 = tf.layers.conv2d(D_h1, 256, 3, strides=(2,2), padding=\"same\", name=\"conv2\",\n",
    "            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(factor=v),\n",
    "            bias_initializer=tf.contrib.layers.variance_scaling_initializer(factor=v))#[?,4,4,256]\n",
    "        D_h2 = leaky_relu(D_h2, alpha)\n",
    "        #D_h2 = tf.contrib.layers.batch_norm(D_h2)\n",
    "\n",
    "        D_h3 = tf.layers.conv2d(D_h2, 256, 3, strides=(2,2), padding=\"same\", name=\"conv3\",\n",
    "            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(factor=v),\n",
    "            bias_initializer=tf.contrib.layers.variance_scaling_initializer(factor=v))#[?,2,2,512]\n",
    "\n",
    "        D_out = tf.reduce_mean(D_h3,axis=range(len(D_h3.get_shape()))[1:])#[?,]\n",
    "        return D_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Summaries:\\nwith tf.variable_scope(\"train_summary\"):\\n    tf.summary.scalar(\\'xent_loss\\', loss)\\n    tf.summary.scalar(\\'accuracy\\', accuracy)\\n    train_merged = tf.summary.merge_all()\\n\\nwith tf.variable_scope(\"test_summary\"):\\n    tf.summary.scalar(\\'xent_loss\\', loss)\\n    tf.summary.scalar(\\'accuracy\\', accuracy)\\n    test_merged = tf.summary.merge_all()\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------------------------#\n",
    "# Setting up the Computation Graph #\n",
    "#----------------------------------#\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "z = tf.placeholder(dtype=tf.float32, shape=(None,d), name='z') # to sample: np.random.normal(d)\n",
    "fake_images = generator(z,reuse=False)\n",
    "true_images = tf.placeholder(dtype=tf.float32, shape=BATCH_IN_SHAPE, name='x')\n",
    "with tf.name_scope(\"x_hat\"):\n",
    "    btwn_images = (fake_images + true_images)/2\n",
    "\n",
    "fake_score = discriminator(fake_images, reuse=False)\n",
    "true_score = discriminator(true_images, reuse=True)\n",
    "btwn_score = discriminator(btwn_images, reuse=True)\n",
    "\n",
    "with tf.name_scope(\"D_loss\"):\n",
    "    punishment = lam * (tf.norm(tf.gradients(btwn_score,btwn_images)) - 1)**2 # How close real images are to fake ones?\n",
    "    D_loss = tf.reduce_mean(fake_score - true_score + punishment)\n",
    "with tf.name_scope(\"G_loss\"):\n",
    "    G_loss = -tf.reduce_mean(fake_score)\n",
    "    \n",
    "# Training Optimizers:\n",
    "t_vars = tf.trainable_variables()\n",
    "G_vars = [var for var in t_vars if 'G_' in var.name]\n",
    "D_vars = [var for var in t_vars if 'D_' in var.name]\n",
    "train_D = tf.train.AdamOptimizer(learning_rate=c, name='adam_D', beta1=beta1, beta2=beta2).minimize(D_loss)\n",
    "train_G = tf.train.AdamOptimizer(learning_rate=c, name='adam_G', beta1=beta1, beta2=beta2).minimize(G_loss)\n",
    "\n",
    "\"\"\"\n",
    "# Summaries:\n",
    "with tf.variable_scope(\"train_summary\"):\n",
    "    tf.summary.scalar('xent_loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    train_merged = tf.summary.merge_all()\n",
    "\n",
    "with tf.variable_scope(\"test_summary\"):\n",
    "    tf.summary.scalar('xent_loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    test_merged = tf.summary.merge_all()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#----------#\n",
    "# Training #\n",
    "#----------#\n",
    "\n",
    "num_batches = 6000\n",
    "print_after = 60\n",
    "\n",
    "z_test = np.random.normal(size=(1,d))\n",
    "progress = [] #progressive development of a single z input\n",
    "\n",
    "#For interpolation:\n",
    "z1 = np.random.normal(size=d)\n",
    "z2 = np.random.normal(size=d)\n",
    "z10 = zip(*[np.linspace(z1[i],z2[i],10) for i in range(d)])\n",
    "\n",
    "#-----------------------------------------------------------------------------------#\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#train_writer = tf.summary.FileWriter(\"./tf_logs\", sess.graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tD_loss:  0.160461807251 \tG_loss:  -2.76886275969e-05\n",
      "60 \tD_loss:  4.04604619344 \tG_loss:  -0.619700749715\n",
      "120 \tD_loss:  32.448449707 \tG_loss:  -2.66366424561\n",
      "180 \tD_loss:  63.1638509115 \tG_loss:  -3.22280375163\n",
      "240 \tD_loss:  54.7000935872 \tG_loss:  -2.64044672648\n",
      "300 \tD_loss:  68.9684895833 \tG_loss:  -3.54525604248\n",
      "360 \tD_loss:  119.669230143 \tG_loss:  -4.87718048096\n",
      "420 \tD_loss:  261.412532552 \tG_loss:  -11.5265808105\n",
      "480 \tD_loss:  586.076171875 \tG_loss:  -23.351969401\n",
      "540 \tD_loss:  1181.64427083 \tG_loss:  -41.4013224284\n",
      "600 \tD_loss:  4567.31458333 \tG_loss:  -60.9448160807\n",
      "660 \tD_loss:  7117.63645833 \tG_loss:  -145.009830729\n",
      "720 \tD_loss:  11574.321875 \tG_loss:  -203.248876953\n",
      "780 \tD_loss:  16131.0989583 \tG_loss:  -247.75296224\n"
     ]
    }
   ],
   "source": [
    "for t in xrange(num_batches):#(N/S):\n",
    "#for t in np.ones(num_batches,dtype=np.int32): # Overfitting test\n",
    "    d_avg = 0\n",
    "    g_avg = 0\n",
    "    z_in = np.random.normal(size=(S,d))\n",
    "    x_indeces = np.random.randint(N,size=S)\n",
    "    _, _, d_loss, g_loss = sess.run([train_D, train_G, D_loss, G_loss],\n",
    "        feed_dict={true_images:features[x_indeces], z:z_in})\n",
    "    #train_writer.add_summary(graph_summary, t)\n",
    "    d_avg += d_loss\n",
    "    g_avg += g_loss\n",
    "    if t % print_after == 0:\n",
    "        print t, \"\\tD_loss: \", d_avg/print_after, \"\\tG_loss: \", g_avg/print_after\n",
    "        #print t, \"\\tD_loss: \", d_loss, \"\\tG_loss: \", g_loss\n",
    "        d_avg = 0\n",
    "        g_avg = 0\n",
    "    if t % (num_batches/10) == 0:\n",
    "        img = sess.run(fake_images, feed_dict={z:z_test})\n",
    "        progress.append(img[0])\n",
    "\n",
    "#train_writer.close()\n",
    "\n",
    "# Build Progressive Strip:\n",
    "progression = np.concatenate(progress,axis=1)\n",
    "progression = np.maximum(0.0,np.minimum(1.0,progression))\n",
    "progression = np.uint8(progression*255)\n",
    "\n",
    "# Bulid Examples Multi-Image:\n",
    "examples = sess.run(fake_images, feed_dict={z:np.random.normal(size=(100,d))})\n",
    "im10 = np.concatenate([examples[range(i,i+10)] for i in range(0,100,10)],axis=1)\n",
    "im100 = np.concatenate(im10,axis=1)\n",
    "im100 = np.maximum(0.0,np.minimum(1.0,im100))\n",
    "im100 = np.uint8(im100*255)\n",
    "\n",
    "# Build Interpolation:\n",
    "interpoles = sess.run(fake_images, feed_dict={z:z10})\n",
    "interpolated = np.concatenate(interpoles,axis=1)\n",
    "interpolated = np.maximum(0.0,np.minimum(1.0,interpolated))\n",
    "interpolated = np.uint8(interpolated*255)\n",
    "\n",
    "imsave(\"progression.png\",progression)\n",
    "imsave(\"multi-image.png\",im100)\n",
    "imsave(\"interpolation.png\",interpolated)\n",
    "\n",
    "display(PILImage.open('progression.png'))\n",
    "display(PILImage.open('multi-image.png'))\n",
    "display(PILImage.open('interpolation.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
