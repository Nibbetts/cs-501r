{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Baseline is: 0.500000\n"
     ]
    }
   ],
   "source": [
    "# Nathan Tibbetts\n",
    "# Lab 6 - Cancer Detection\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "#from IPython.display import display, Image\n",
    "\n",
    "TRAIN_PATH = \"cancer_data/train\"\n",
    "TEST_PATH = \"cancer_data/test\"\n",
    "# Load Data:\n",
    "train_in_files  = [f for f in listdir(join(TRAIN_PATH, \"inputs\"))  if isfile(join(join(TRAIN_PATH, \"inputs\"),  f))]\n",
    "train_out_files = [f for f in listdir(join(TRAIN_PATH, \"outputs\")) if isfile(join(join(TRAIN_PATH, \"outputs\"), f))]\n",
    "test_in_files   = [f for f in listdir(join(TEST_PATH, \"inputs\"))   if isfile(join(join(TEST_PATH, \"inputs\"),   f))]\n",
    "test_out_files  = [f for f in listdir(join(TEST_PATH, \"outputs\"))  if isfile(join(join(TEST_PATH, \"outputs\"),  f))]\n",
    "\n",
    "assert (train_in_files == train_out_files), \"Mismatch in direcory contents between train-in and train-out files!\"\n",
    "assert (test_in_files == test_out_files), \"Mismatch in directory contents between test-in and test-out files!\"\n",
    "N = len(train_in_files) # Number of Test images to use.\n",
    "T = len(test_in_files) # Number of Train images to use.\n",
    "S = 5 # Mini-batch size - shared for both train and test.\n",
    "IN_SHAPE = (64, 64, 3) # Used only to scale down the images we work with - change for final trainings!\n",
    "OUT_SHAPE = (64, 64) # Match back end to front end, but only one channel.\n",
    "\n",
    "train_neg = [f for f in train_in_files if f[:3] == \"neg\"]\n",
    "train_pos = [f for f in train_in_files if f[:3] == \"pos\"]\n",
    "test_neg  = [f for f in test_in_files  if f[:3] == \"neg\"]\n",
    "test_pos  = [f for f in test_in_files  if f[:3] == \"pos\"]\n",
    "assert (len(train_neg) + len(train_pos) == len(train_in_files)), \"Not all train files identifiable as 'pos' or 'neg'!\"\n",
    "assert (len(test_neg)  + len(test_pos)  == len(test_in_files)),  \"Not all test files identifiable as 'pos' or 'neg'!\"\n",
    "\n",
    "print \"Accuracy Baseline is: %f\" % ((len(train_pos)/float(len(train_in_files)) * len(test_pos) +\n",
    "                                    len(train_neg)/float(len(train_in_files)) * len(test_neg)) / len(test_in_files))\n",
    "#     % we say pos. * num real pos. cases + % we say neg. * num real neg. cases, all / by num. cases to make a %.\n",
    " \n",
    "features      = np.array([resize(imread(join(TRAIN_PATH, \"inputs\",  f)), IN_SHAPE)\n",
    "                          for f in train_in_files[:N]],  dtype='float32')\n",
    "labels        = np.array([resize(imread(join(TRAIN_PATH, \"outputs\", f)), OUT_SHAPE)\n",
    "                          for f in train_in_files[:N]], dtype='int32')\n",
    "test_features = np.array([resize(imread(join(TEST_PATH,  \"inputs\",  f)), IN_SHAPE)\n",
    "                          for f in test_in_files[:T]],   dtype='float32')\n",
    "test_labels   = np.array([resize(imread(join(TEST_PATH,  \"outputs\", f)), OUT_SHAPE)\n",
    "                          for f in test_in_files[:T]],  dtype='int32')\n",
    "# Note: Above we're not using train_out_files or test_out_files because we have asserted they are the same,\n",
    "#       and if we shuffle them we want them to still be paired.\n",
    "\n",
    "# plt.imshow(labels[0])\n",
    "\n",
    "# Whiten our data - zero mean and unit standard deviation\n",
    "mean = np.mean(features)#, axis=0) # I think the best way to put these on the same page is to scale them the same,\n",
    "std  = np.std(features)#, axis=0)  #     which is based on the scaling for our features we know about beforehand.\n",
    "features      = (features      - mean) / std # REMOVED axis=0 ... MAKES SENSE ???\n",
    "test_features = (test_features - mean) / std\n",
    "# features = np.transpose(np.reshape(features,(N,3,32,32)),axes=[0,2,3,1])\n",
    "# features is (N, 1024, 1024, 3), or (batch_size, height, width, channels)\n",
    "\n",
    "#plt.imshow(features[N-1]*0.1 + 0.5)\n",
    "\n",
    "HEIGHT = np.shape(features)[1]\n",
    "WIDTH = np.shape(features)[2]\n",
    "CHANNELS = np.shape(features)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer Defining helper functions\n",
    "\n",
    "# Convolution Layer:\n",
    "def conv( x, filter_size=3, stride=2, num_filters=64, is_output=False, name=\"conv\" ):\n",
    "    '''\n",
    "    x is an input tensor\n",
    "    Declare a name scope using the \"name\" parameter\n",
    "    Within that scope:\n",
    "      Create a W filter variable with the proper size\n",
    "      Create a B bias variable with the proper size\n",
    "      Convolve x with W by calling the tf.nn.conv2d function\n",
    "      Add the bias\n",
    "      If is_output is False,\n",
    "        Call the tf.nn.relu function\n",
    "      Return the final op\n",
    "    ''' \n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable(\"W\",\n",
    "                            shape=(filter_size,\n",
    "                                   filter_size,\n",
    "                                   x.get_shape().as_list()[3],\n",
    "                                   num_filters),\n",
    "                            initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        b = tf.get_variable(\"b\",\n",
    "                            shape=num_filters,\n",
    "                            initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        # Notice the input x will be one minibatch at a time,\n",
    "        #    but right now we're just building a computation graph!\n",
    "        h = tf.nn.conv2d(x, W, [1,stride,stride,1], padding=\"SAME\", name=\"h\")\n",
    "        h = tf.nn.bias_add(h, b, name=\"h\")\n",
    "        if not is_output:\n",
    "            h = tf.nn.relu(h, name=\"h_hat\")\n",
    "        return h;        \n",
    "\n",
    "# Fully-Connected Layer:\n",
    "def fc( x, out_size=50, is_output=False, name=\"fc\" ):\n",
    "    '''\n",
    "    x is an input tensor\n",
    "    Declare a name scope using the \"name\" parameter\n",
    "    Within that scope:\n",
    "      Create a W filter variable with the proper size\n",
    "      Create a B bias variable with the proper size\n",
    "      Multiply x by W and add b\n",
    "      If is_output is False,\n",
    "        Call the tf.nn.relu function\n",
    "      Return the final op\n",
    "    '''\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable(\"W\",\n",
    "                            shape=(out_size,x.get_shape().as_list()[0]),\n",
    "                            initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        b = tf.get_variable(\"b\",\n",
    "                            shape=(out_size,1),#out_size,\n",
    "                            initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        h = tf.matmul(W, x, name=\"h\") + b\n",
    "        #h = tf.nn.bias_add(h, b, name=\"h\")#(tf.reshape(h,[-1]), b, name=\"h\")\n",
    "        if not is_output:\n",
    "            h = tf.nn.relu(h, name=\"h_hat\")\n",
    "        return h;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Setting up the Computation Graph\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(dtype=np.float32,shape=(S,HEIGHT,WIDTH,CHANNELS),name='x')\n",
    "y = tf.placeholder(dtype=np.int32,shape=(S,HEIGHT,WIDTH),name='y')\n",
    "x_test = tf.placeholder(dtype=np.float32, shape=(S,HEIGHT,WIDTH,CHANNELS), name=\"x_test\")\n",
    "y_test = tf.placeholder(dtype=np.int32, shape=(S,HEIGHT,WIDTH), name=\"y_test\")\n",
    "#lambda 0.01 - 0.05\n",
    "h0 = conv(x,stride=1,name='conv1',num_filters=2,is_output=True) # Shape h0 = [S,128,128,2]\n",
    "#DROPOUT btw layers - Layers not nn, because is wrapper, and has training=True when not test\n",
    "#h1 = conv(h0,stride=1,name='conv2')\n",
    "#h2 = conv(h1,stride=2,name='conv3_stride2') # Shape h2 = [S,16,16,64]\n",
    "#h3 = conv(h2,stride=1,name='conv4')\n",
    "#h4 = conv(h3,stride=2,name='conv5_stride2',num_filters=48) # Shape h4 = [S,8,8,48]\n",
    "#h5 = conv(h4,stride=2,name='conv6_stride2',num_filters=48) # Shape h5 = [S,4,4,48]\n",
    "#h6 = conv(h5,stride=1,name='conv7',is_output=True,num_filters=32) # Shape h6 = [S,4,4,32]\n",
    "\n",
    "shape1 = h0.get_shape().as_list()#h6.get_shape().as_list()\n",
    "# flat1 = tf.transpose(tf.reshape(h6,[shape1[0],shape1[1]*shape1[2]*shape1[3]]))\n",
    "# flat1 is [512,S]\n",
    "# fc1 = fc(flat1,out_size=64,name='fc1')# fc1 is [64,S]\n",
    "# fc2 = fc(fc1,out_size=10,name='fc2',is_output=True) # fc2 is [10,S]\n",
    "\n",
    "#h1b = conv(h0,stride=2,name='conv2b') # Shape h1b -> [S,16,16,64]\n",
    "#h2b = conv(h1b,stride=2,name='conv3b') # Shape h2b -> [S,8,8,64]\n",
    "#h3b = conv(h2b,stride=2,name='conv4b',is_output=True) # Shape h3b -> [S,4,4,64]\n",
    "#shape2 = h3b.get_shape().as_list()\n",
    "#flat2 = tf.transpose(tf.reshape(h3b,[shape2[0],shape2[1]*shape2[2]*shape2[3]]))\n",
    "## flat2 is [1024,S]\n",
    "#fc1b = fc(flat2,out_size=32)#,is_output=True)\n",
    "## fc1b is [32,S]\n",
    "#h6 = tf.concat([fc1,fc1b],axis=0,name='combined')\n",
    "#fc2 = fc(h6,out_size=10,name='fc2',is_output=True) # fc2 is [10,S]\n",
    "\n",
    "with tf.name_scope(name=\"train_loss\") as scope:\n",
    "    xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=h0,\n",
    "        name='cross_entropy')\n",
    "    loss = tf.reduce_mean(xent)\n",
    "    \n",
    "#with tf.name_scope(name=\"test_loss\") as scope:\n",
    "#    test_xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "#        labels=y,\n",
    "#        logits=tf.transpose(fc2),\n",
    "#        name='cross_entropy')\n",
    "#    test_loss = tf.reduce_mean(xent)\n",
    "\n",
    "with tf.name_scope(name=\"accuracy\") as scope:\n",
    "    prediction = tf.cast(tf.argmax(h0,axis=-1),tf.int32)#tf.one_hot(tf.argmax(fc2,axis=0),10)\n",
    "    #acc = tf.reduce_sum(tf.abs(y-prediction),name='accuracy')#1-(0.5*(1.0/S)*....)\n",
    "    correct_prediction = tf.equal(prediction,y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    #accuracy = tf.metrics.accuracy(y,prediction,name=\"acc\")\n",
    "    \n",
    "#with tf.name_scope(name=\"test_accuracy\") as scope:\n",
    "#    test_prediction = tf.argmax(h0,axis=0)\n",
    "#    test_correct_prediction = tf.equal(prediction,y)\n",
    "#    test_accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "c = 0.001 # Learning Rate\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=c,name='adam_optimizer').minimize(xent)\n",
    "#test_step = tf.train.AdamOptimizer(learning_rate=0,name='adam_tester').minimize(test_xent)\n",
    "\n",
    "\n",
    "#xx = features[:10]\n",
    "#yy = labels[:10]\n",
    "\n",
    "with tf.variable_scope(\"train\"):\n",
    "    tf.summary.scalar('xent_loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    train_merged = tf.summary.merge_all()\n",
    "\n",
    "with tf.variable_scope(\"test\"):\n",
    "    tf.summary.scalar('xent_loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    test_merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.145061 0.971631\n",
      "1 0.601822 0.811523\n",
      "2 0.5545 0.829541\n",
      "3 0.0840609 0.985937\n",
      "4 0.776772 0.751025\n",
      "5 0.135028 0.968164\n",
      "6 0.340207 0.826123\n",
      "7 0.113907 0.975586\n",
      "8 0.534782 0.834033\n",
      "9 0.763986 0.754883\n",
      "10 0.855998 0.738965\n",
      "11 0.334887 0.902051\n",
      "12 0.474712 0.849121\n",
      "13 0.391193 0.883447\n",
      "14 0.488366 0.855322\n",
      "15 0.133169 0.97168\n",
      "16 0.216385 0.943555\n",
      "17 0.27689 0.920508\n",
      "18 0.572136 0.823828\n",
      "19 0.104491 0.981104\n",
      "20 0.112126 0.982178\n",
      "21 0.158781 0.963721\n",
      "22 0.711547 0.775928\n",
      "23 0.11111 0.977979\n",
      "24 1.61776 0.45791\n",
      "25 0.973835 0.689795\n",
      "26 0.607494 0.70874\n",
      "27 0.304169 0.790771\n",
      "28 0.564037 0.818066\n",
      "29 1.16821 0.56626\n",
      "30 0.658769 0.789258\n",
      "31 0.823115 0.741455\n",
      "32 0.285564 0.920557\n",
      "33 0.430054 0.867139\n",
      "34 0.700369 0.777246\n",
      "35 0.791299 0.635742\n",
      "36 0.560239 0.814746\n",
      "37 0.359118 0.899854\n",
      "38 0.852409 0.729541\n",
      "39 0.418165 0.871924\n",
      "40 0.88453 0.731006\n",
      "41 0.387099 0.867383\n",
      "42 0.530402 0.828564\n",
      "43 0.405967 0.879785\n",
      "44 0.486132 0.843115\n",
      "45 0.972276 0.671582\n",
      "46 0.500962 0.845459\n",
      "47 0.241223 0.96167\n",
      "48 0.608189 0.834521\n",
      "49 0.104686 0.983057\n",
      "50 0.253216 0.923193\n",
      "51 0.246329 0.931738\n",
      "52 0.552438 0.837598\n",
      "53 0.4015 0.873389\n",
      "54 0.120361 0.975684\n",
      "55 0.119965 0.971094\n",
      "56 0.298172 0.908398\n",
      "57 0.275041 0.918848\n",
      "58 0.885397 0.695166\n",
      "59 0.461684 0.847607\n",
      "60 0.121595 0.985986\n",
      "61 0.576015 0.806982\n",
      "62 0.835082 0.718701\n",
      "63 0.973556 0.676953\n",
      "64 0.548532 0.821094\n",
      "65 0.699462 0.756592\n",
      "66 0.462183 0.867041\n",
      "67 0.744567 0.753516\n",
      "68 0.96912 0.658691\n",
      "69 0.764148 0.72832\n",
      "70 0.10067 0.992871\n",
      "71 0.128145 0.978711\n",
      "72 0.728644 0.750879\n",
      "73 1.33285 0.508154\n",
      "74 0.986399 0.647021\n",
      "75 0.832789 0.686133\n",
      "76 0.122892 0.975293\n",
      "77 0.480097 0.82041\n",
      "78 0.532309 0.828076\n",
      "79 0.557004 0.799365\n",
      "80 0.673734 0.759717\n",
      "81 0.183138 0.926953\n",
      "82 0.391692 0.879492\n",
      "83 0.886824 0.67124\n",
      "84 0.613367 0.79873\n",
      "85 1.1281 0.536621\n",
      "86 0.302274 0.819238\n",
      "87 0.440219 0.795996\n",
      "88 0.197641 0.935449\n",
      "89 0.424062 0.776025\n",
      "90 0.794909 0.730176\n",
      "91 0.283899 0.902393\n",
      "92 0.196018 0.89707\n",
      "93 0.406901 0.866064\n",
      "94 0.482716 0.819482\n",
      "95 0.449935 0.838086\n",
      "96 0.117727 0.982324\n",
      "97 0.129533 0.971338\n",
      "98 0.303552 0.91709\n",
      "99 0.989268 0.637109\n",
      "100 0.223791 0.934424\n",
      "101 0.433433 0.836426\n",
      "102 0.695995 0.761328\n",
      "103 1.09592 0.584912\n",
      "104 0.296858 0.904541\n",
      "105 0.284287 0.901758\n",
      "106 0.51479 0.793652\n",
      "107 0.441998 0.81792\n",
      "108 0.568788 0.795605\n",
      "109 0.34895 0.890381\n",
      "110 0.952072 0.590234\n",
      "111 0.732816 0.692871\n",
      "112 0.174199 0.950146\n",
      "113 0.799388 0.688672\n",
      "114 0.306868 0.819141\n",
      "115 0.298024 0.912354\n",
      "116 0.63622 0.757666\n",
      "117 0.190689 0.946582\n",
      "118 0.149024 0.969092\n",
      "119 0.740842 0.732715\n",
      "120 0.187766 0.934912\n",
      "121 0.408226 0.836133\n",
      "122 0.755341 0.701465\n",
      "123 0.583289 0.790625\n",
      "124 0.592844 0.779102\n",
      "125 0.200123 0.844775\n",
      "126 0.800161 0.683252\n",
      "127 0.514788 0.803418\n",
      "128 0.816044 0.558154\n",
      "129 0.324013 0.88042\n",
      "130 0.315768 0.891748\n",
      "131 0.60801 0.755469\n",
      "132 0.150231 0.965332\n",
      "133 0.146026 0.963965\n",
      "134 0.432356 0.839551\n",
      "135 0.680103 0.799316\n",
      "136 0.34592 0.876318\n",
      "137 0.469584 0.812353\n",
      "138 0.23133 0.932861\n",
      "139 0.44857 0.822705\n",
      "140 0.382297 0.860986\n",
      "141 0.421204 0.849609\n",
      "142 0.33618 0.878857\n",
      "143 0.414738 0.849512\n",
      "144 0.14832 0.965234\n",
      "145 0.409582 0.837158\n",
      "146 0.350627 0.88457\n",
      "147 0.360689 0.864453\n",
      "148 0.327821 0.874951\n",
      "149 0.362094 0.867432\n",
      "150 0.125229 0.973877\n",
      "151 0.357931 0.890918\n",
      "152 0.179563 0.945557\n",
      "153 0.256191 0.92168\n",
      "154 0.208964 0.940771\n",
      "155 0.245308 0.95293\n",
      "156 0.198462 0.95752\n",
      "157 0.45379 0.833105\n",
      "158 0.358846 0.871729\n",
      "159 0.174551 0.947266\n",
      "160 0.216658 0.935596\n",
      "161 0.467763 0.823926\n",
      "162 0.394585 0.858252\n",
      "163 0.295319 0.899414\n",
      "164 0.215877 0.942236\n",
      "165 0.733224 0.67915\n",
      "166 0.418039 0.848535\n",
      "167 0.267553 0.91748\n",
      "168 0.303385 0.906055\n",
      "169 0.424617 0.866016\n",
      "170 0.644755 0.719189\n",
      "171 0.455591 0.812891\n",
      "172 0.295272 0.897217\n",
      "173 0.156155 0.962061\n",
      "174 0.472112 0.829639\n",
      "175 0.640047 0.759229\n",
      "176 0.616212 0.765478\n",
      "177 0.772814 0.658447\n",
      "178 0.161895 0.958398\n",
      "179 0.556933 0.753955\n",
      "180 0.719549 0.716016\n",
      "181 0.0857512 0.989453\n",
      "182 0.45414 0.846875\n",
      "183 0.100997 0.986279\n",
      "184 0.115366 0.999805\n",
      "185 0.162255 0.957227\n",
      "186 0.492204 0.813135\n",
      "187 0.218274 0.927979\n",
      "188 0.420726 0.863623\n",
      "189 0.0885734 0.99082\n",
      "190 0.330398 0.89668\n",
      "191 0.361336 0.861133\n",
      "192 0.332287 0.870508\n",
      "193 0.371845 0.83208\n",
      "194 0.248374 0.920654\n",
      "195 0.362941 0.857031\n",
      "196 0.319311 0.889111\n",
      "197 0.31908 0.895703\n",
      "198 0.143035 0.971338\n",
      "199 0.314978 0.897217\n",
      "200 0.0942094 0.998096\n",
      "201 0.176242 0.952197\n",
      "202 0.421662 0.858154\n",
      "203 0.522668 0.792969\n",
      "204 0.676787 0.718994\n",
      "205 0.172353 0.957031\n",
      "206 0.343767 0.868506\n",
      "207 0.162917 0.954639\n",
      "208 0.42858 0.828223\n",
      "209 0.270026 0.902588\n",
      "210 0.288187 0.913037\n",
      "211 0.252841 0.922412\n",
      "212 0.220045 0.936572\n",
      "213 0.19385 0.929785\n",
      "214 0.181503 0.941064\n",
      "215 0.787143 0.69624\n",
      "216 0.179484 0.944238\n",
      "217 0.771406 0.712256\n",
      "218 0.576518 0.764453\n",
      "219 0.646676 0.73623\n",
      "220 0.20756 0.932861\n",
      "221 0.418626 0.850195\n",
      "222 0.250038 0.931201\n",
      "223 0.0968226 0.98418\n",
      "224 0.283254 0.891455\n",
      "225 0.275526 0.905908\n",
      "226 0.134642 0.963525\n",
      "227 0.644374 0.756396\n",
      "228 0.145191 0.967969\n",
      "229 0.65479 0.748633\n",
      "230 0.435764 0.84834\n",
      "231 0.480756 0.818018\n",
      "232 0.525683 0.80376\n",
      "233 0.485159 0.803027\n",
      "234 0.102163 0.996191\n",
      "235 0.28374 0.900732\n",
      "236 0.62234 0.720654\n",
      "237 0.376059 0.845361\n",
      "238 0.386855 0.836865\n",
      "239 0.135841 0.972314\n",
      "240 0.885169 0.626953\n",
      "241 0.338084 0.877979\n",
      "242 0.365489 0.879785\n",
      "243 0.547074 0.83833\n",
      "244 0.594321 0.757959\n",
      "245 0.387934 0.866943\n",
      "246 0.120367 0.983105\n",
      "247 0.402103 0.845508\n",
      "248 0.344468 0.87041\n",
      "249 0.59101 0.74873\n",
      "250 0.805904 0.67168\n",
      "251 0.430388 0.856299\n",
      "252 0.224225 0.917285\n",
      "253 0.125323 0.975342\n",
      "254 0.187528 0.929736\n",
      "255 0.351383 0.876953\n",
      "256 0.651315 0.745264\n",
      "257 0.718336 0.730127\n",
      "258 0.34358 0.875342\n",
      "259 0.601272 0.748682\n",
      "260 0.269568 0.933398\n",
      "261 0.115883 0.982422\n",
      "262 0.246833 0.926074\n",
      "263 0.390247 0.848779\n",
      "264 0.263939 0.908496\n",
      "265 0.22934 0.921973\n",
      "266 0.332233 0.891943\n",
      "267 0.21104 0.948828\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter(\"./tf_logs\", sess.graph)\n",
    "    \n",
    "    #for t in xrange(S,100*S,S):#(S,D+S,S):\n",
    "    #for t in np.ones(100,dtype=np.int32)*32:\n",
    "    for t in xrange(N/S):\n",
    "    #for t in np.ones(15,dtype=np.int32): # Overfitting test\n",
    "        ix = np.random.randint(N,size=S)\n",
    "        _, loss_value, acc, graph_summary = sess.run(\n",
    "            [train_step, loss, accuracy, train_merged],\n",
    "            #feed_dict={x:features[S*t:S*(t+1)],y:labels[S*t:S*(t+1)]})\n",
    "            #feed_dict={x:[features[n] for n in np.random.randint(N,size=S)],\n",
    "            #           y:[labels[n]   for n in np.random.randint(N,size=S)]})\n",
    "            feed_dict={x:features[ix],y:labels[ix]})\n",
    "        train_writer.add_summary(graph_summary, t)\n",
    "        if t%S == 0:\n",
    "            ix = np.random.randint(T,size=S)\n",
    "            tt = t/S\n",
    "            test_loss_value, test_acc, test_summary = sess.run(\n",
    "                [loss, accuracy, test_merged],\n",
    "                #feed_dict={x:test_features[S*tt:S*(tt+1)], y:test_labels[S*tt:S*(tt+1)]})\n",
    "                feed_dict={x:test_features[ix],y:test_labels[ix]})\n",
    "            train_writer.add_summary(test_summary, t)\n",
    "        print t, loss_value, acc\n",
    "        #print test_loss, test_acc\n",
    "        \n",
    "    train_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import PIL.Image as PILImage\n",
    "# from IPython.display import display, Image\n",
    "\n",
    "# display(PILImage.open('lab5_comp_graph'))\n",
    "# display(PILImage.open('lab5_graphs.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
