{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Baseline is: 0.500000\n"
     ]
    }
   ],
   "source": [
    "# Nathan Tibbetts\n",
    "# Lab 6 - Cancer Detection\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "#from IPython.display import display, Image\n",
    "\n",
    "TRAIN_PATH = \"cancer_data/train\"\n",
    "TEST_PATH = \"cancer_data/test\"\n",
    "# Load Data:\n",
    "train_in_files  = [f for f in listdir(join(TRAIN_PATH, \"inputs\"))  if isfile(join(join(TRAIN_PATH, \"inputs\"),  f))]\n",
    "train_out_files = [f for f in listdir(join(TRAIN_PATH, \"outputs\")) if isfile(join(join(TRAIN_PATH, \"outputs\"), f))]\n",
    "test_in_files   = [f for f in listdir(join(TEST_PATH, \"inputs\"))   if isfile(join(join(TEST_PATH, \"inputs\"),   f))]\n",
    "test_out_files  = [f for f in listdir(join(TEST_PATH, \"outputs\"))  if isfile(join(join(TEST_PATH, \"outputs\"),  f))]\n",
    "\n",
    "assert (train_in_files == train_out_files), \"Mismatch in direcory contents between train-in and train-out files!\"\n",
    "assert (test_in_files == test_out_files), \"Mismatch in directory contents between test-in and test-out files!\"\n",
    "N=20#N = len(train_in_files) # Number of Test images to use.\n",
    "T=5#T = len(test_in_files) # Number of Train images to use.\n",
    "S = 5 # Mini-batch size\n",
    "IN_SHAPE = (128, 128, 3) # Used only to scale down the images we work with - change for final trainings!\n",
    "OUT_SHAPE = (128, 128) # Match back end to front end, but only one channel.\n",
    "\n",
    "train_neg = [f for f in train_in_files if f[:3] == \"neg\"]\n",
    "train_pos = [f for f in train_in_files if f[:3] == \"pos\"]\n",
    "test_neg  = [f for f in test_in_files  if f[:3] == \"neg\"]\n",
    "test_pos  = [f for f in test_in_files  if f[:3] == \"pos\"]\n",
    "assert (len(train_neg) + len(train_pos) == len(train_in_files)), \"Not all train files identifiable as 'pos' or 'neg'!\"\n",
    "assert (len(test_neg)  + len(test_pos)  == len(test_in_files)),  \"Not all test files identifiable as 'pos' or 'neg'!\"\n",
    "\n",
    "print \"Accuracy Baseline is: %f\" % ((len(train_pos)/float(len(train_in_files)) * len(test_pos) +\n",
    "                                    len(train_neg)/float(len(train_in_files)) * len(test_neg)) / len(test_in_files))\n",
    "#     % we say pos. * num real pos. cases + % we say neg. * num real neg. cases, all / by num. cases to make a %.\n",
    " \n",
    "features      = np.array([resize(imread(join(TRAIN_PATH, \"inputs\",  f)), IN_SHAPE)\n",
    "                          for f in train_in_files[:N]],  dtype='float32')\n",
    "labels        = np.array([resize(imread(join(TRAIN_PATH, \"outputs\", f)), OUT_SHAPE)\n",
    "                          for f in train_out_files[:N]], dtype='float32')\n",
    "test_features = np.array([resize(imread(join(TEST_PATH,  \"inputs\",  f)), IN_SHAPE)\n",
    "                          for f in test_in_files[:T]],   dtype='float32')\n",
    "test_labels   = np.array([resize(imread(join(TEST_PATH,  \"outputs\", f)), OUT_SHAPE)\n",
    "                          for f in test_out_files[:T]],  dtype='float32')\n",
    "\n",
    "#plt.imshow(features[0])\n",
    "\n",
    "# Whiten our data - zero mean and unit standard deviation\n",
    "mean = np.mean(features)#, axis=0) # I think the best way to put these on the same page is to scale them the same,\n",
    "std  = np.std(features)#, axis=0)  #     which is based on the scaling for our features we know about beforehand.\n",
    "features      = (features      - mean) / std # REMOVED axis=0 ... MAKES SENSE ???\n",
    "test_features = (test_features - mean) / std\n",
    "# features = np.transpose(np.reshape(features,(N,3,32,32)),axes=[0,2,3,1])\n",
    "# features is (N, 1024, 1024, 3), or (batch_size, height, width, channels)\n",
    "\n",
    "#plt.imshow(features[N-1]*0.1 + 0.5)\n",
    "\n",
    "HEIGHT = np.shape(features)[1]\n",
    "WIDTH = np.shape(features)[2]\n",
    "CHANNELS = np.shape(features)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer Defining helper functions\n",
    "\n",
    "# Convolution Layer:\n",
    "def conv( x, filter_size=3, stride=2, num_filters=64, is_output=False, name=\"conv\" ):\n",
    "    '''\n",
    "    x is an input tensor\n",
    "    Declare a name scope using the \"name\" parameter\n",
    "    Within that scope:\n",
    "      Create a W filter variable with the proper size\n",
    "      Create a B bias variable with the proper size\n",
    "      Convolve x with W by calling the tf.nn.conv2d function\n",
    "      Add the bias\n",
    "      If is_output is False,\n",
    "        Call the tf.nn.relu function\n",
    "      Return the final op\n",
    "    ''' \n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable(\"W\",\n",
    "                            shape=(filter_size,\n",
    "                                   filter_size,\n",
    "                                   x.get_shape().as_list()[3],\n",
    "                                   num_filters),\n",
    "                            initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        b = tf.get_variable(\"b\",\n",
    "                            shape=num_filters,\n",
    "                            initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        # Notice the input x will be one minibatch at a time,\n",
    "        #    but right now we're just building a computation graph!\n",
    "        h = tf.nn.conv2d(x, W, [1,stride,stride,1], padding=\"SAME\", name=\"h\")\n",
    "        h = tf.nn.bias_add(h, b, name=\"h\")\n",
    "        if not is_output:\n",
    "            h = tf.nn.relu(h, name=\"h_hat\")\n",
    "        return h;        \n",
    "\n",
    "# Fully-Connected Layer:\n",
    "def fc( x, out_size=50, is_output=False, name=\"fc\" ):\n",
    "    '''\n",
    "    x is an input tensor\n",
    "    Declare a name scope using the \"name\" parameter\n",
    "    Within that scope:\n",
    "      Create a W filter variable with the proper size\n",
    "      Create a B bias variable with the proper size\n",
    "      Multiply x by W and add b\n",
    "      If is_output is False,\n",
    "        Call the tf.nn.relu function\n",
    "      Return the final op\n",
    "    '''\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable(\"W\",\n",
    "                            shape=(out_size,x.get_shape().as_list()[0]),\n",
    "                            initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        b = tf.get_variable(\"b\",\n",
    "                            shape=(out_size,1),#out_size,\n",
    "                            initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        h = tf.matmul(W, x, name=\"h\") + b\n",
    "        #h = tf.nn.bias_add(h, b, name=\"h\")#(tf.reshape(h,[-1]), b, name=\"h\")\n",
    "        if not is_output:\n",
    "            h = tf.nn.relu(h, name=\"h_hat\")\n",
    "        return h;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(dtype=np.float32,shape=(S,HEIGHT,WIDTH,CHANNELS),name='x')\n",
    "y = tf.placeholder(dtype=np.int64,shape=(S),name='y')\n",
    "#test = tf.placeholder(dtype=np.float32,shape=(S,HEIGHT,WIDTH,CHANNELS),name='test_data')\n",
    "\n",
    "h0 = conv(x,stride=1,name='conv1') # Shape h0 = [S,32,32,64]\n",
    "h1 = conv(h0,stride=1,name='conv2')\n",
    "h2 = conv(h1,stride=2,name='conv3_stride2') # Shape h2 = [S,16,16,64]\n",
    "h3 = conv(h2,stride=1,name='conv4')\n",
    "h4 = conv(h3,stride=2,name='conv5_stride2',num_filters=48) # Shape h4 = [S,8,8,48]\n",
    "h5 = conv(h4,stride=2,name='conv6_stride2',num_filters=48) # Shape h5 = [S,4,4,48]\n",
    "h6 = conv(h5,stride=1,name='conv7',is_output=True,num_filters=32) # Shape h6 = [S,4,4,32]\n",
    "\n",
    "shape1 = h6.get_shape().as_list()\n",
    "flat1 = tf.transpose(tf.reshape(h6,[shape1[0],shape1[1]*shape1[2]*shape1[3]]))\n",
    "# flat1 is [512,S]\n",
    "fc1 = fc(flat1,out_size=64,name='fc1')# fc1 is [64,S]\n",
    "fc2 = fc(fc1,out_size=10,name='fc2',is_output=True) # fc2 is [10,S]\n",
    "\n",
    "#h1b = conv(h0,stride=2,name='conv2b') # Shape h1b -> [S,16,16,64]\n",
    "#h2b = conv(h1b,stride=2,name='conv3b') # Shape h2b -> [S,8,8,64]\n",
    "#h3b = conv(h2b,stride=2,name='conv4b',is_output=True) # Shape h3b -> [S,4,4,64]\n",
    "#shape2 = h3b.get_shape().as_list()\n",
    "#flat2 = tf.transpose(tf.reshape(h3b,[shape2[0],shape2[1]*shape2[2]*shape2[3]]))\n",
    "## flat2 is [1024,S]\n",
    "#fc1b = fc(flat2,out_size=32)#,is_output=True)\n",
    "## fc1b is [32,S]\n",
    "#h6 = tf.concat([fc1,fc1b],axis=0,name='combined')\n",
    "#fc2 = fc(h6,out_size=10,name='fc2',is_output=True) # fc2 is [10,S]\n",
    "\n",
    "with tf.name_scope(name=\"train_loss\") as scope:\n",
    "    xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=tf.transpose(fc2),\n",
    "        name='cross_entropy')\n",
    "    loss = tf.reduce_mean(xent)\n",
    "    \n",
    "with tf.name_scope(name=\"test_loss\") as scope:\n",
    "    test_xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=tf.transpose(fc2),\n",
    "        name='cross_entropy')\n",
    "    test_loss = tf.reduce_mean(xent)\n",
    "\n",
    "with tf.name_scope(name=\"train_accuracy\") as scope:\n",
    "    prediction = tf.argmax(fc2,axis=0)#tf.one_hot(tf.argmax(fc2,axis=0),10)\n",
    "    #acc = tf.reduce_sum(tf.abs(y-prediction),name='accuracy')#1-(0.5*(1.0/S)*....)\n",
    "    correct_prediction = tf.equal(prediction,y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    #accuracy = tf.metrics.accuracy(y,prediction,name=\"acc\")\n",
    "    \n",
    "with tf.name_scope(name=\"test_accuracy\") as scope:\n",
    "    test_prediction = tf.argmax(fc2,axis=0)\n",
    "    test_correct_prediction = tf.equal(prediction,y)\n",
    "    test_accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "c = 0.001 # Learning Rate\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=c,name='adam_optimizer').minimize(xent)\n",
    "test_step = tf.train.AdamOptimizer(learning_rate=0,name='adam_tester').minimize(test_xent)\n",
    "\n",
    "\n",
    "#xx = features[:10]\n",
    "#yy = labels[:10]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter(\"./tf_logs\", sess.graph)\n",
    "    \n",
    "    tf.summary.scalar('xent_loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('test_xent_loss', test_loss)\n",
    "    tf.summary.scalar('test_accuracy', test_accuracy)\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    #for t in xrange(S,100*S,S):#(S,D+S,S):\n",
    "    #for t in np.ones(100,dtype=np.int32)*32:\n",
    "    for t in xrange(D/S):\n",
    "    #for t in np.ones(15,dtype=np.int32): # Overfitting test\n",
    "        _, loss_value, acc, graph_summary = sess.run(\n",
    "            [train_step, loss, accuracy, merged],\n",
    "            feed_dict={x:features[S*t:S*(t+1)],y:labels[S*t:S*(t+1)]})\n",
    "        train_writer.add_summary(graph_summary, t)\n",
    "        if t%(S/T) == 0:\n",
    "            tt = t/S/T\n",
    "            _, test_loss_value, test_acc, test_summary = sess.run(\n",
    "                [test_step, test_loss, test_accuracy, merged],\n",
    "                feed_dict={x:features[N + S*tt:N + S*(tt+1)], y:labels[N + S*tt:N + S*(tt+1)]})\n",
    "            train_writer.add_summary(test_summary, t)\n",
    "        print t, loss_value, acc\n",
    "        #print test_loss, test_acc\n",
    "        \n",
    "train_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import PIL.Image as PILImage\n",
    "# from IPython.display import display, Image\n",
    "\n",
    "# display(PILImage.open('lab5_comp_graph'))\n",
    "# display(PILImage.open('lab5_graphs.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
