{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Baseline is: 0.500000\n"
     ]
    }
   ],
   "source": [
    "# Nathan Tibbetts\n",
    "# Lab 6 - Cancer Detection\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "#from IPython.display import display, Image\n",
    "\n",
    "TRAIN_PATH = \"cancer_data/train\"\n",
    "TEST_PATH = \"cancer_data/test\"\n",
    "# Load Data:\n",
    "train_in_files  = [f for f in listdir(join(TRAIN_PATH, \"inputs\"))  if isfile(join(join(TRAIN_PATH, \"inputs\"),  f))]\n",
    "train_out_files = [f for f in listdir(join(TRAIN_PATH, \"outputs\")) if isfile(join(join(TRAIN_PATH, \"outputs\"), f))]\n",
    "test_in_files   = [f for f in listdir(join(TEST_PATH, \"inputs\"))   if isfile(join(join(TEST_PATH, \"inputs\"),   f))]\n",
    "test_out_files  = [f for f in listdir(join(TEST_PATH, \"outputs\"))  if isfile(join(join(TEST_PATH, \"outputs\"),  f))]\n",
    "\n",
    "assert (train_in_files == train_out_files), \"Mismatch in direcory contents between train-in and train-out files!\"\n",
    "assert (test_in_files == test_out_files), \"Mismatch in directory contents between test-in and test-out files!\"\n",
    "N = len(train_in_files) # Number of Test images to use.\n",
    "T = len(test_in_files) # Number of Train images to use.\n",
    "S = 5 # Mini-batch size - shared for both train and test.\n",
    "IN_SHAPE = (64, 64, 3) # Used only to scale down the images we work with - change for final trainings!\n",
    "OUT_SHAPE = (64, 64) # Match back end to front end, but only one channel.\n",
    "\n",
    "train_neg = [f for f in train_in_files if f[:3] == \"neg\"]\n",
    "train_pos = [f for f in train_in_files if f[:3] == \"pos\"]\n",
    "test_neg  = [f for f in test_in_files  if f[:3] == \"neg\"]\n",
    "test_pos  = [f for f in test_in_files  if f[:3] == \"pos\"]\n",
    "assert (len(train_neg) + len(train_pos) == len(train_in_files)), \"Not all train files identifiable as 'pos' or 'neg'!\"\n",
    "assert (len(test_neg)  + len(test_pos)  == len(test_in_files)),  \"Not all test files identifiable as 'pos' or 'neg'!\"\n",
    "\n",
    "print \"Accuracy Baseline is: %f\" % ((len(train_pos)/float(len(train_in_files)) * len(test_pos) +\n",
    "                                    len(train_neg)/float(len(train_in_files)) * len(test_neg)) / len(test_in_files))\n",
    "#     % we say pos. * num real pos. cases + % we say neg. * num real neg. cases, all / by num. cases to make a %.\n",
    " \n",
    "features      = np.array([resize(imread(join(TRAIN_PATH, \"inputs\",  f)), IN_SHAPE)\n",
    "                          for f in train_in_files[:N]],  dtype='float32')\n",
    "labels        = np.array([resize(imread(join(TRAIN_PATH, \"outputs\", f)), OUT_SHAPE)\n",
    "                          for f in train_in_files[:N]], dtype='int32')\n",
    "test_features = np.array([resize(imread(join(TEST_PATH,  \"inputs\",  f)), IN_SHAPE)\n",
    "                          for f in test_in_files[:T]],   dtype='float32')\n",
    "test_labels   = np.array([resize(imread(join(TEST_PATH,  \"outputs\", f)), OUT_SHAPE)\n",
    "                          for f in test_in_files[:T]],  dtype='int32')\n",
    "# Note: Above we're not using train_out_files or test_out_files because we have asserted they are the same,\n",
    "#       and if we shuffle them we want them to still be paired.\n",
    "\n",
    "# plt.imshow(labels[0])\n",
    "\n",
    "# Whiten our data - zero mean and unit standard deviation\n",
    "mean = np.mean(features)#, axis=0) # I think the best way to put these on the same page is to scale them the same,\n",
    "std  = np.std(features)#, axis=0)  #     which is based on the scaling for our features we know about beforehand.\n",
    "features      = (features      - mean) / std # REMOVED axis=0 ... MAKES SENSE ???\n",
    "test_features = (test_features - mean) / std\n",
    "# features = np.transpose(np.reshape(features,(N,3,32,32)),axes=[0,2,3,1])\n",
    "# features is (N, 1024, 1024, 3), or (batch_size, height, width, channels)\n",
    "\n",
    "#plt.imshow(features[N-1]*0.1 + 0.5)\n",
    "\n",
    "HEIGHT = np.shape(features)[1]\n",
    "WIDTH = np.shape(features)[2]\n",
    "CHANNELS = np.shape(features)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer Defining helper functions\n",
    "\n",
    "# Convolution Layer:\n",
    "def conv( x, filter_size=3, stride=2, num_filters=64, is_output=False, name=\"conv\" ):\n",
    "    '''\n",
    "    x is an input tensor\n",
    "    Declare a name scope using the \"name\" parameter\n",
    "    Within that scope:\n",
    "      Create a W filter variable with the proper size\n",
    "      Create a B bias variable with the proper size\n",
    "      Convolve x with W by calling the tf.nn.conv2d function\n",
    "      Add the bias\n",
    "      If is_output is False,\n",
    "        Call the tf.nn.relu function\n",
    "      Return the final op\n",
    "    ''' \n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable(\"W\",\n",
    "                            shape=(filter_size,\n",
    "                                   filter_size,\n",
    "                                   x.get_shape().as_list()[3],\n",
    "                                   num_filters),\n",
    "                            initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        b = tf.get_variable(\"b\",\n",
    "                            shape=num_filters,\n",
    "                            initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        # Notice the input x will be one minibatch at a time,\n",
    "        #    but right now we're just building a computation graph!\n",
    "        h = tf.nn.conv2d(x, W, [1,stride,stride,1], padding=\"SAME\", name=\"h\")\n",
    "        h = tf.nn.bias_add(h, b, name=\"h\")\n",
    "        if not is_output:\n",
    "            h = tf.nn.relu(h, name=\"h_hat\")\n",
    "        return h;        \n",
    "\n",
    "# Fully-Connected Layer:\n",
    "def fc( x, out_size=50, is_output=False, name=\"fc\" ):\n",
    "    '''\n",
    "    x is an input tensor\n",
    "    Declare a name scope using the \"name\" parameter\n",
    "    Within that scope:\n",
    "      Create a W filter variable with the proper size\n",
    "      Create a B bias variable with the proper size\n",
    "      Multiply x by W and add b\n",
    "      If is_output is False,\n",
    "        Call the tf.nn.relu function\n",
    "      Return the final op\n",
    "    '''\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable(\"W\",\n",
    "                            shape=(out_size,x.get_shape().as_list()[0]),\n",
    "                            initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        b = tf.get_variable(\"b\",\n",
    "                            shape=(out_size,1),#out_size,\n",
    "                            initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        h = tf.matmul(W, x, name=\"h\") + b\n",
    "        #h = tf.nn.bias_add(h, b, name=\"h\")#(tf.reshape(h,[-1]), b, name=\"h\")\n",
    "        if not is_output:\n",
    "            h = tf.nn.relu(h, name=\"h_hat\")\n",
    "        return h;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Setting up the Computation Graph\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(dtype=np.float32,shape=(S,HEIGHT,WIDTH,CHANNELS),name='x')\n",
    "y = tf.placeholder(dtype=np.int32,shape=(S,HEIGHT,WIDTH),name='y')\n",
    "x_test = tf.placeholder(dtype=np.float32, shape=(S,HEIGHT,WIDTH,CHANNELS), name=\"x_test\")\n",
    "y_test = tf.placeholder(dtype=np.int32, shape=(S,HEIGHT,WIDTH), name=\"y_test\")\n",
    "#lambda 0.01 - 0.05\n",
    "h0 = conv(x,stride=1,name='conv1',num_filters=2,is_output=True) # Shape h0 = [S,128,128,2]\n",
    "#DROPOUT btw layers - Layers not nn, because is wrapper, and has training=True when not test\n",
    "#h1 = conv(h0,stride=1,name='conv2')\n",
    "#h2 = conv(h1,stride=2,name='conv3_stride2') # Shape h2 = [S,16,16,64]\n",
    "#h3 = conv(h2,stride=1,name='conv4')\n",
    "#h4 = conv(h3,stride=2,name='conv5_stride2',num_filters=48) # Shape h4 = [S,8,8,48]\n",
    "#h5 = conv(h4,stride=2,name='conv6_stride2',num_filters=48) # Shape h5 = [S,4,4,48]\n",
    "#h6 = conv(h5,stride=1,name='conv7',is_output=True,num_filters=32) # Shape h6 = [S,4,4,32]\n",
    "\n",
    "shape1 = h0.get_shape().as_list()#h6.get_shape().as_list()\n",
    "# flat1 = tf.transpose(tf.reshape(h6,[shape1[0],shape1[1]*shape1[2]*shape1[3]]))\n",
    "# flat1 is [512,S]\n",
    "# fc1 = fc(flat1,out_size=64,name='fc1')# fc1 is [64,S]\n",
    "# fc2 = fc(fc1,out_size=10,name='fc2',is_output=True) # fc2 is [10,S]\n",
    "\n",
    "#h1b = conv(h0,stride=2,name='conv2b') # Shape h1b -> [S,16,16,64]\n",
    "#h2b = conv(h1b,stride=2,name='conv3b') # Shape h2b -> [S,8,8,64]\n",
    "#h3b = conv(h2b,stride=2,name='conv4b',is_output=True) # Shape h3b -> [S,4,4,64]\n",
    "#shape2 = h3b.get_shape().as_list()\n",
    "#flat2 = tf.transpose(tf.reshape(h3b,[shape2[0],shape2[1]*shape2[2]*shape2[3]]))\n",
    "## flat2 is [1024,S]\n",
    "#fc1b = fc(flat2,out_size=32)#,is_output=True)\n",
    "## fc1b is [32,S]\n",
    "#h6 = tf.concat([fc1,fc1b],axis=0,name='combined')\n",
    "#fc2 = fc(h6,out_size=10,name='fc2',is_output=True) # fc2 is [10,S]\n",
    "\n",
    "with tf.name_scope(name=\"train_loss\") as scope:\n",
    "    xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=h0,\n",
    "        name='cross_entropy')\n",
    "    loss = tf.reduce_mean(xent)\n",
    "    \n",
    "#with tf.name_scope(name=\"test_loss\") as scope:\n",
    "#    test_xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "#        labels=y,\n",
    "#        logits=tf.transpose(fc2),\n",
    "#        name='cross_entropy')\n",
    "#    test_loss = tf.reduce_mean(xent)\n",
    "\n",
    "with tf.name_scope(name=\"accuracy\") as scope:\n",
    "    prediction = tf.cast(tf.argmax(h0,axis=-1),tf.int32)#tf.one_hot(tf.argmax(fc2,axis=0),10)\n",
    "    #acc = tf.reduce_sum(tf.abs(y-prediction),name='accuracy')#1-(0.5*(1.0/S)*....)\n",
    "    correct_prediction = tf.equal(prediction,y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    #accuracy = tf.metrics.accuracy(y,prediction,name=\"acc\")\n",
    "    \n",
    "#with tf.name_scope(name=\"test_accuracy\") as scope:\n",
    "#    test_prediction = tf.argmax(h0,axis=0)\n",
    "#    test_correct_prediction = tf.equal(prediction,y)\n",
    "#    test_accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "c = 0.001 # Learning Rate\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=c,name='adam_optimizer').minimize(xent)\n",
    "#test_step = tf.train.AdamOptimizer(learning_rate=0,name='adam_tester').minimize(test_xent)\n",
    "\n",
    "\n",
    "#xx = features[:10]\n",
    "#yy = labels[:10]\n",
    "\n",
    "with tf.variable_scope(\"train\"):\n",
    "    tf.summary.scalar('xent_loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    train_merged = tf.summary.merge_all()\n",
    "\n",
    "with tf.variable_scope(\"test\"):\n",
    "    tf.summary.scalar('xent_loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    test_merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.853242 0.685059\n",
      "1 1.02838 0.588623\n",
      "2 1.65003 0.637451\n",
      "3 0.643704 0.714746\n",
      "4 0.681202 0.80415\n",
      "5 1.11801 0.560303\n",
      "6 2.03151 0.535449\n",
      "7 0.862871 0.64502\n",
      "8 0.676371 0.736719\n",
      "9 0.946723 0.672412\n",
      "10 0.939861 0.547754\n",
      "11 0.778197 0.686377\n",
      "12 0.874903 0.593603\n",
      "13 1.11012 0.576514\n",
      "14 0.826635 0.757129\n",
      "15 0.921193 0.617676\n",
      "16 0.903186 0.684668\n",
      "17 0.927528 0.677734\n",
      "18 0.649145 0.782812\n",
      "19 0.810358 0.726074\n",
      "20 0.945107 0.653223\n",
      "21 0.66761 0.73667\n",
      "22 1.02317 0.668555\n",
      "23 1.11206 0.589111\n",
      "24 0.666395 0.723096\n",
      "25 0.943271 0.583838\n",
      "26 1.20785 0.5375\n",
      "27 0.793616 0.696094\n",
      "28 0.809882 0.5604\n",
      "29 0.905668 0.654932\n",
      "30 1.03492 0.610986\n",
      "31 1.08529 0.442578\n",
      "32 0.84592 0.651221\n",
      "33 1.38487 0.470654\n",
      "34 1.17056 0.49502\n",
      "35 0.878932 0.698193\n",
      "36 0.759284 0.662256\n",
      "37 0.702994 0.789551\n",
      "38 0.923797 0.634863\n",
      "39 0.830039 0.678467\n",
      "40 1.14797 0.467334\n",
      "41 0.901823 0.603076\n",
      "42 0.934293 0.560107\n",
      "43 1.20206 0.59502\n",
      "44 0.889643 0.655908\n",
      "45 1.21429 0.574219\n",
      "46 1.10628 0.621191\n",
      "47 0.767284 0.729102\n",
      "48 0.81712 0.652051\n",
      "49 0.847481 0.585693\n",
      "50 0.845742 0.673047\n",
      "51 0.909457 0.529443\n",
      "52 1.00378 0.408398\n",
      "53 0.792398 0.734766\n",
      "54 0.961774 0.507227\n",
      "55 1.20514 0.455762\n",
      "56 0.797766 0.57583\n",
      "57 0.817949 0.638379\n",
      "58 0.879656 0.59668\n",
      "59 0.957202 0.555908\n",
      "60 0.706204 0.703857\n",
      "61 0.792885 0.712207\n",
      "62 0.881194 0.648438\n",
      "63 0.855613 0.615332\n",
      "64 0.695868 0.752441\n",
      "65 0.786229 0.586719\n",
      "66 0.742041 0.702686\n",
      "67 0.742871 0.672559\n",
      "68 0.756666 0.663867\n",
      "69 0.898152 0.580518\n",
      "70 0.869079 0.63252\n",
      "71 0.811029 0.568555\n",
      "72 0.760545 0.658057\n",
      "73 0.906587 0.603857\n",
      "74 0.826428 0.551465\n",
      "75 0.871012 0.536621\n",
      "76 0.862737 0.559473\n",
      "77 0.80369 0.714258\n",
      "78 1.12278 0.438525\n",
      "79 0.759006 0.680957\n",
      "80 0.905614 0.596631\n",
      "81 0.932141 0.451318\n",
      "82 0.814515 0.664111\n",
      "83 0.865251 0.562353\n",
      "84 0.692561 0.745508\n",
      "85 0.794465 0.66123\n",
      "86 0.857403 0.633154\n",
      "87 0.816459 0.57207\n",
      "88 0.861465 0.530371\n",
      "89 0.805242 0.614941\n",
      "90 0.797136 0.650342\n",
      "91 0.723082 0.727051\n",
      "92 0.851511 0.626611\n",
      "93 0.735708 0.675439\n",
      "94 0.807483 0.596094\n",
      "95 0.882727 0.60332\n",
      "96 0.90742 0.589697\n",
      "97 0.827856 0.652588\n",
      "98 0.651409 0.7375\n",
      "99 0.855121 0.56958\n",
      "100 0.920926 0.544141\n",
      "101 0.812786 0.649609\n",
      "102 0.755998 0.650488\n",
      "103 0.733211 0.729688\n",
      "104 0.782731 0.644238\n",
      "105 0.901168 0.516748\n",
      "106 0.792843 0.672607\n",
      "107 0.743216 0.709375\n",
      "108 0.714876 0.745898\n",
      "109 0.734272 0.68291\n",
      "110 0.729373 0.652344\n",
      "111 0.832519 0.499316\n",
      "112 0.730406 0.704004\n",
      "113 0.656934 0.756787\n",
      "114 0.802122 0.678516\n",
      "115 0.788615 0.663037\n",
      "116 0.803976 0.632715\n",
      "117 1.00784 0.595068\n",
      "118 0.799404 0.556348\n",
      "119 0.791898 0.669092\n",
      "120 0.795547 0.67749\n",
      "121 0.774511 0.731201\n",
      "122 0.678487 0.795459\n",
      "123 0.72976 0.732715\n",
      "124 0.808988 0.624023\n",
      "125 0.720444 0.693066\n",
      "126 0.681844 0.766357\n",
      "127 0.815108 0.592676\n",
      "128 0.625813 0.80791\n",
      "129 0.77765 0.649951\n",
      "130 0.878809 0.694482\n",
      "131 0.671757 0.783936\n",
      "132 0.68256 0.801318\n",
      "133 0.851242 0.495459\n",
      "134 0.941707 0.698096\n",
      "135 0.663915 0.791553\n",
      "136 0.780464 0.671191\n",
      "137 0.74214 0.638037\n",
      "138 0.654918 0.749658\n",
      "139 0.770126 0.611279\n",
      "140 0.857513 0.567383\n",
      "141 0.800309 0.580225\n",
      "142 0.74775 0.62207\n",
      "143 0.683341 0.701563\n",
      "144 0.645842 0.78667\n",
      "145 0.705438 0.744971\n",
      "146 0.660799 0.739795\n",
      "147 0.707698 0.695947\n",
      "148 0.715469 0.796631\n",
      "149 0.917352 0.569141\n",
      "150 0.820582 0.57124\n",
      "151 0.878969 0.610937\n",
      "152 0.836405 0.641211\n",
      "153 0.705092 0.737939\n",
      "154 0.656051 0.752783\n",
      "155 0.781721 0.682666\n",
      "156 0.741873 0.728955\n",
      "157 0.905008 0.557666\n",
      "158 0.6831 0.698486\n",
      "159 0.747725 0.717871\n",
      "160 0.705287 0.723975\n",
      "161 0.718949 0.741406\n",
      "162 0.674936 0.757568\n",
      "163 0.889839 0.524365\n",
      "164 0.835102 0.499365\n",
      "165 0.598098 0.804443\n",
      "166 0.625113 0.775781\n",
      "167 0.850997 0.575781\n",
      "168 0.719836 0.728613\n",
      "169 0.665852 0.652783\n",
      "170 0.876584 0.629834\n",
      "171 0.954197 0.621631\n",
      "172 0.80079 0.626807\n",
      "173 0.865541 0.696777\n",
      "174 0.64501 0.762207\n",
      "175 0.677796 0.752148\n",
      "176 0.679571 0.731201\n",
      "177 0.933564 0.623828\n",
      "178 0.718071 0.696924\n",
      "179 0.795275 0.568701\n",
      "180 0.789555 0.60459\n",
      "181 0.795609 0.613232\n",
      "182 0.699428 0.743701\n",
      "183 0.828486 0.68457\n",
      "184 0.697237 0.700146\n",
      "185 0.873988 0.549805\n",
      "186 0.660377 0.765527\n",
      "187 0.753375 0.647949\n",
      "188 0.713629 0.704297\n",
      "189 0.834647 0.599902\n",
      "190 0.710511 0.671728\n",
      "191 0.767729 0.640234\n",
      "192 0.753893 0.656543\n",
      "193 0.661321 0.7521\n",
      "194 0.6537 0.743164\n",
      "195 0.752739 0.59043\n",
      "196 0.720902 0.715625\n",
      "197 0.677076 0.763525\n",
      "198 0.706381 0.734131\n",
      "199 0.608451 0.818213\n",
      "200 0.698532 0.735498\n",
      "201 0.617872 0.847559\n",
      "202 0.730754 0.697705\n",
      "203 0.654359 0.785645\n",
      "204 0.797204 0.619629\n",
      "205 0.7008 0.732666\n",
      "206 0.651465 0.772949\n",
      "207 0.680792 0.755225\n",
      "208 0.637938 0.791895\n",
      "209 0.804689 0.635986\n",
      "210 0.798278 0.634424\n",
      "211 0.60399 0.817432\n",
      "212 0.784389 0.657666\n",
      "213 0.646359 0.806006\n",
      "214 0.715182 0.700049\n",
      "215 0.647341 0.779248\n",
      "216 0.772646 0.608545\n",
      "217 0.712513 0.71001\n",
      "218 0.767648 0.665576\n",
      "219 0.726055 0.667285\n",
      "220 0.782876 0.57998\n",
      "221 0.702818 0.687207\n",
      "222 0.700824 0.73125\n",
      "223 0.708278 0.667432\n",
      "224 0.719807 0.606689\n",
      "225 0.65952 0.77207\n",
      "226 0.688257 0.725293\n",
      "227 0.873655 0.477295\n",
      "228 0.604537 0.802295\n",
      "229 0.660925 0.759326\n",
      "230 0.906687 0.393652\n",
      "231 0.764349 0.58667\n",
      "232 0.665066 0.780225\n",
      "233 0.764845 0.555713\n",
      "234 0.638203 0.751611\n",
      "235 0.6385 0.789697\n",
      "236 0.661716 0.764502\n",
      "237 0.704549 0.656006\n",
      "238 0.721385 0.672412\n",
      "239 0.662391 0.696875\n",
      "240 0.687391 0.74624\n",
      "241 0.741797 0.610547\n",
      "242 0.63932 0.761084\n",
      "243 0.787836 0.622461\n",
      "244 0.680209 0.702978\n",
      "245 0.67237 0.754346\n",
      "246 0.643509 0.816895\n",
      "247 0.675763 0.738672\n",
      "248 0.656873 0.744482\n",
      "249 0.693886 0.634668\n",
      "250 0.831562 0.572021\n",
      "251 0.642297 0.768652\n",
      "252 0.650041 0.82002\n",
      "253 0.71146 0.697363\n",
      "254 0.677822 0.759668\n",
      "255 0.710507 0.704736\n",
      "256 0.625594 0.701416\n",
      "257 0.770211 0.584033\n",
      "258 0.735934 0.621875\n",
      "259 0.676867 0.722998\n",
      "260 0.655882 0.741846\n",
      "261 0.695205 0.656592\n",
      "262 0.631129 0.782227\n",
      "263 0.718991 0.691846\n",
      "264 0.658565 0.748438\n",
      "265 0.622249 0.779639\n",
      "266 0.594889 0.82251\n",
      "267 0.679644 0.720264\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter(\"./tf_logs\", sess.graph)\n",
    "    \n",
    "    #for t in xrange(S,100*S,S):#(S,D+S,S):\n",
    "    #for t in np.ones(100,dtype=np.int32)*32:\n",
    "    for t in xrange(N/S):\n",
    "    #for t in np.ones(15,dtype=np.int32): # Overfitting test\n",
    "        ix = np.random.randint(N,size=S)\n",
    "        _, loss_value, acc, graph_summary = sess.run(\n",
    "            [train_step, loss, accuracy, train_merged],\n",
    "            #feed_dict={x:features[S*t:S*(t+1)],y:labels[S*t:S*(t+1)]})\n",
    "            #feed_dict={x:[features[n] for n in np.random.randint(N,size=S)],\n",
    "            #           y:[labels[n]   for n in np.random.randint(N,size=S)]})\n",
    "            feed_dict={x:features[ix],y:labels[ix]})\n",
    "        train_writer.add_summary(graph_summary, t)\n",
    "        if t%S == 0:\n",
    "            ix = np.random.randint(T,size=S)\n",
    "            tt = t/S\n",
    "            test_loss_value, test_acc, test_summary = sess.run(\n",
    "                [loss, accuracy, test_merged],\n",
    "                #feed_dict={x:test_features[S*tt:S*(tt+1)], y:test_labels[S*tt:S*(tt+1)]})\n",
    "                feed_dict={x:test_features[ix],y:test_labels[ix]})\n",
    "            train_writer.add_summary(test_summary, t)\n",
    "        print t, loss_value, acc\n",
    "        #print test_loss, test_acc\n",
    "        \n",
    "    train_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import PIL.Image as PILImage\n",
    "# from IPython.display import display, Image\n",
    "\n",
    "# display(PILImage.open('lab5_comp_graph'))\n",
    "# display(PILImage.open('lab5_graphs.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
